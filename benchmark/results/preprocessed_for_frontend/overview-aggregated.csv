Model name,Mean
gpt-4,0.9002757352941176
gpt-3.5-turbo,0.8854166666666667
llama-2-chat:7:ggmlv3:q2_K,0.5372242647058824
llama-2-chat:7:ggmlv3:q3_K_M,0.53515625
llama-2-chat:7:ggmlv3:q5_0,0.4739583333333333
llama-2-chat:13:ggmlv3:q5_0,0.4609375
llama-2-chat:7:ggmlv3:q4_0,0.44140625
llama-2-chat:7:ggmlv3:q4_K_M,0.43046875
llama-2-chat:13:ggmlv3:q4_1,0.4296875
llama-2-chat:7:ggmlv3:q5_K_M,0.42421875
llama-2-chat:13:ggmlv3:q4_0,0.4140625
llama-2-chat:7:ggmlv3:q6_K,0.403125
llama-2-chat:7:ggmlv3:q4_1,0.403125
mixtral-instruct-v0.1:46_7:ggufv2:Q2_K,0.3984375
llama-2-chat:13:ggmlv3:q5_K_M,0.3984375
llama-2-chat:13:ggmlv3:q6_K,0.3776041666666667
llama-2-chat:13:ggmlv3:q8_0,0.3776041666666667
llama-2-chat:7:ggmlv3:q4_K_S,0.3697916666666667
mixtral-instruct-v0.1:46_7:ggufv2:Q4_0,0.359375
llama-2-chat:13:ggmlv3:q4_K_S,0.3528645833333333
llama-2-chat:7:ggmlv3:q8_0,0.34765625
mixtral-instruct-v0.1:46_7:ggufv2:Q5_0,0.3333333333333333
llama-2-chat:13:ggmlv3:q2_K,0.33035714285714285
mixtral-instruct-v0.1:46_7:ggufv2:Q8_0,0.328125
llama-2-chat:13:ggmlv3:q4_K_M,0.3151041666666667
mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M,0.3020833333333333
llama-2-chat:13:ggmlv3:q3_K_M,0.23958333333333331
