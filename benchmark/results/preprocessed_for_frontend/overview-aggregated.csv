Model name,Size,Quantisation,Mean
gpt-3.5-turbo,,,0.8777573529411764
llama-2-chat,7,q3_K_M,0.51640625
llama-2-chat,7,q5_0,0.4864583333333333
llama-2-chat,7,q5_K_M,0.465625
llama-2-chat,13,q5_0,0.4609375
llama-2-chat,7,q4_0,0.45
llama-2-chat,7,q4_K_S,0.4447916666666667
llama-2-chat,13,q4_1,0.4296875
llama-2-chat,7,q4_1,0.415625
llama-2-chat,13,q4_0,0.4078125
llama-2-chat,7,q4_K_M,0.4075520833333333
llama-2-chat,7,q8_0,0.403125
llama-2-chat,13,q5_K_M,0.3984375
llama-2-chat,13,q4_K_S,0.3802083333333333
llama-2-chat,13,q8_0,0.3776041666666667
llama-2-chat,13,q4_K_M,0.3776041666666667
llama-2-chat,13,q6_K,0.3776041666666667
llama-2-chat,7,q6_K,0.35703125
mixtral-instruct-v0.1,46_7,Q2_K,0.35026041666666663
mixtral-instruct-v0.1,46_7,Q4_0,0.34895833333333337
mixtral-instruct-v0.1,46_7,Q4_K_M,0.34791666666666665
llama-2-chat,13,q3_K_M,0.3359375
mixtral-instruct-v0.1,46_7,Q5_0,0.33125
llama-2-chat,7,q2_K,0.325765931372549
llama-2-chat,13,q2_K,0.2994791666666667
mixtral-instruct-v0.1,46_7,Q8_0,0.28125
