Model name,Size,Version,Quantisation,Mean Accuracy,SD
openhermes-2.5,7,ggufv2,Q8_0,1.0,0.0
llama-2-chat,70,ggufv2,Q4_K_M,1.0,0.0
openhermes-2.5,7,ggufv2,Q4_K_M,1.0,0.0
gpt-4,Unknown,,,1.0,0.0
gpt-3.5-turbo,175,,,0.9,0.18
llama-2-chat,70,ggufv2,Q5_K_M,0.88,0.12
llama-2-chat,7,ggufv2,Q5_K_M,0.88,0.12
llama-2-chat,13,ggufv2,Q4_K_S,0.88,0.12
llama-2-chat,70,ggufv2,Q2_K,0.75,0.25
llama-2-chat,13,ggufv2,Q8_0,0.75,0.25
llama-2-chat,13,ggufv2,Q6_K,0.75,0.25
llama-2-chat,13,ggufv2,Q4_0,0.75,0.25
llama-2-chat,13,ggufv2,Q4_1,0.75,0.25
llama-2-chat,13,ggufv2,Q5_0,0.75,0.25
llama-2-chat,7,ggufv2,Q4_0,0.75,0.25
llama-2-chat,70,ggufv2,Q3_K_M,0.75,0.25
llama-2-chat,7,ggufv2,Q3_K_M,0.75,0.25
llama-2-chat,7,ggufv2,Q4_K_M,0.75,0.25
llama-2-chat,7,ggufv2,Q4_K_S,0.75,0.25
llama-2-chat,7,ggufv2,Q5_0,0.75,0.25
llama-2-chat,7,ggufv2,Q6_K,0.75,0.25
llama-2-chat,7,ggufv2,Q8_0,0.75,0.25
openhermes-2.5,7,ggufv2,Q2_K,0.75,0.25
llama-2-chat,7,ggufv2,Q4_1,0.75,0.25
mixtral-instruct-v0.1,"46,7",ggufv2,Q4_0,0.71,0.29
llama-2-chat,7,ggufv2,Q2_K,0.67,0.17
mixtral-instruct-v0.1,"46,7",ggufv2,Q4_K_M,0.58,0.42
mixtral-instruct-v0.1,"46,7",ggufv2,Q2_K,0.54,0.21
mixtral-instruct-v0.1,"46,7",ggufv2,Q6_K,0.5,0.5
mixtral-instruct-v0.1,"46,7",ggufv2,Q8_0,0.42,0.33
llama-2-chat,13,ggufv2,Q4_K_M,0.41,0.42
llama-2-chat,13,ggufv2,Q5_K_M,0.4,0.37
llama-2-chat,13,ggufv2,Q3_K_M,0.39,0.37
mixtral-instruct-v0.1,"46,7",ggufv2,Q5_0,0.29,0.21
llama-2-chat,13,ggufv2,Q2_K,0.28,0.37
mixtral-instruct-v0.1,"46,7",ggufv2,Q3_K_M,0.25,0.25
