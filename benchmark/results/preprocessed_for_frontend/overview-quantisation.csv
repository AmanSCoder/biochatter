Model name,Size,Version,Quantisation,Median Accuracy,SD
gpt-3.5-turbo,175,,,1.0,0.18
gpt-4,Unknown,,,1.0,0.12
llama-2-chat,7,ggufv2,Q5_K_M,0.88,0.12
openhermes-2.5,7,ggufv2,Q8_0,0.88,0.34
llama-2-chat,70,ggufv2,Q5_K_M,0.88,0.12
llama-2-chat,13,ggufv2,Q4_K_S,0.88,0.12
openhermes-2.5,7,ggufv2,Q4_K_M,0.8,0.39
code-llama-instruct,7,ggufv2,Q4_K_M,0.75,0.43
llama-2-chat,13,ggufv2,Q5_K_M,0.75,0.25
llama-2-chat,7,ggufv2,Q3_K_M,0.75,0.25
llama-2-chat,7,ggufv2,Q4_0,0.75,0.25
llama-2-chat,7,ggufv2,Q4_1,0.75,0.25
llama-2-chat,7,ggufv2,Q4_K_S,0.75,0.25
llama-2-chat,7,ggufv2,Q5_0,0.75,0.25
llama-2-chat,13,ggufv2,Q5_0,0.75,0.25
llama-2-chat,7,ggufv2,Q6_K,0.75,0.25
llama-2-chat,13,ggufv2,Q6_K,0.75,0.25
llama-2-chat,13,ggufv2,Q4_1,0.75,0.25
llama-2-chat,13,ggufv2,Q4_0,0.75,0.25
llama-2-chat,70,ggufv2,Q4_K_M,0.75,0.38
llama-2-chat,70,ggufv2,Q3_K_M,0.75,0.25
mixtral-instruct-v0.1,"46,7",ggufv2,Q4_0,0.71,0.29
llama-2-chat,13,ggufv2,Q3_K_M,0.67,0.17
chatglm3,6,ggmlv3,q4_0,0.5,0.35
llama-2-chat,7,ggufv2,Q4_K_M,0.5,0.36
llama-2-chat,70,ggufv2,Q2_K,0.5,0.41
code-llama-instruct,34,ggufv2,Q2_K,0.5,0.39
openhermes-2.5,7,ggufv2,Q2_K,0.5,0.39
llama-2-chat,7,ggufv2,Q8_0,0.5,0.37
mixtral-instruct-v0.1,"46,7",ggufv2,Q6_K,0.5,0.5
code-llama-instruct,13,ggufv2,Q8_0,0.5,0.4
code-llama-instruct,7,ggufv2,Q8_0,0.5,0.4
llama-2-chat,13,ggufv2,Q8_0,0.41,0.38
code-llama-instruct,7,ggufv2,Q2_K,0.38,0.3
code-llama-instruct,13,ggufv2,Q4_K_M,0.33,0.38
mixtral-instruct-v0.1,"46,7",ggufv2,Q2_K,0.33,0.3
code-llama-instruct,34,ggufv2,Q8_0,0.33,0.37
llama-2-chat,13,ggufv2,Q4_K_M,0.31,0.39
mixtral-instruct-v0.1,"46,7",ggufv2,Q5_0,0.29,0.21
code-llama-instruct,34,ggufv2,Q4_K_M,0.25,0.37
mixtral-instruct-v0.1,"46,7",ggufv2,Q4_K_M,0.25,0.35
mixtral-instruct-v0.1,"46,7",ggufv2,Q3_K_M,0.25,0.25
llama-2-chat,7,ggufv2,Q2_K,0.16,0.3
code-llama-instruct,13,ggufv2,Q2_K,0.08,0.38
mixtral-instruct-v0.1,"46,7",ggufv2,Q8_0,0.08,0.34
llama-2-chat,13,ggufv2,Q2_K,0.0,0.38
