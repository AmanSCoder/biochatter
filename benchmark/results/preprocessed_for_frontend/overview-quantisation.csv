Model name,Size,Version,Quantisation,Mean Accuracy,SD
gpt-4,Unknown,,,0.94,0.12
gpt-3.5-turbo,175,,,0.9,0.18
llama-2-chat,70,ggufv2,Q5_K_M,0.88,0.12
llama-2-chat,13,ggufv2,Q4_K_S,0.88,0.12
llama-2-chat,7,ggufv2,Q5_K_M,0.88,0.12
llama-2-chat,13,ggufv2,Q5_0,0.75,0.25
llama-2-chat,7,ggufv2,Q6_K,0.75,0.25
llama-2-chat,13,ggufv2,Q5_K_M,0.75,0.25
llama-2-chat,7,ggufv2,Q3_K_M,0.75,0.25
llama-2-chat,7,ggufv2,Q4_0,0.75,0.25
llama-2-chat,7,ggufv2,Q4_1,0.75,0.25
llama-2-chat,7,ggufv2,Q4_K_S,0.75,0.25
llama-2-chat,7,ggufv2,Q5_0,0.75,0.25
llama-2-chat,70,ggufv2,Q3_K_M,0.75,0.25
llama-2-chat,13,ggufv2,Q6_K,0.75,0.25
llama-2-chat,13,ggufv2,Q4_1,0.75,0.25
llama-2-chat,13,ggufv2,Q4_0,0.75,0.25
openhermes-2.5,7,ggufv2,Q8_0,0.71,0.33
mixtral-instruct-v0.1,"46,7",ggufv2,Q4_0,0.71,0.29
llama-2-chat,13,ggufv2,Q3_K_M,0.67,0.17
openhermes-2.5,7,ggufv2,Q4_K_M,0.62,0.39
llama-2-chat,70,ggufv2,Q4_K_M,0.6,0.38
code-llama-instruct,7,ggufv2,Q4_K_M,0.59,0.43
mixtral-instruct-v0.1,"46,7",ggufv2,Q6_K,0.5,0.5
openhermes-2.5,7,ggufv2,Q2_K,0.47,0.39
code-llama-instruct,7,ggufv2,Q2_K,0.45,0.3
llama-2-chat,7,ggufv2,Q4_K_M,0.45,0.36
code-llama-instruct,7,ggufv2,Q8_0,0.45,0.45
code-llama-instruct,34,ggufv2,Q2_K,0.44,0.42
llama-2-chat,7,ggufv2,Q8_0,0.44,0.37
chatglm3,6,ggmlv3,q4_0,0.43,0.35
code-llama-instruct,13,ggufv2,Q8_0,0.43,0.4
mixtral-instruct-v0.1,"46,7",ggufv2,Q2_K,0.43,0.48
llama-2-chat,70,ggufv2,Q2_K,0.42,0.41
llama-2-chat,13,ggufv2,Q4_K_M,0.41,0.42
llama-2-chat,13,ggufv2,Q8_0,0.4,0.4
code-llama-instruct,34,ggufv2,Q8_0,0.4,0.37
mixtral-instruct-v0.1,"46,7",ggufv2,Q4_K_M,0.38,0.36
code-llama-instruct,13,ggufv2,Q4_K_M,0.37,0.38
code-llama-instruct,34,ggufv2,Q4_K_M,0.33,0.37
llama-2-chat,7,ggufv2,Q2_K,0.32,0.34
llama-2-chat,13,ggufv2,Q2_K,0.31,0.38
code-llama-instruct,13,ggufv2,Q2_K,0.29,0.37
mixtral-instruct-v0.1,"46,7",ggufv2,Q8_0,0.29,0.33
mixtral-instruct-v0.1,"46,7",ggufv2,Q5_0,0.29,0.21
mixtral-instruct-v0.1,"46,7",ggufv2,Q3_K_M,0.25,0.25
